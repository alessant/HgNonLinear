{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetlist = ['InVS13','InVS15','LH10','LyonSchool','SFHH','Thiers13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_networks(data_dir, dataset, n_minutes=5, original_nets=True):\n",
    "    \"\"\"Function that reads the edgelist (t, i, j) and returns\n",
    "    a network aggregated at n_minutes snapshots as a dictionary of nx.Graph()s,\n",
    "    having t as a key.\n",
    "    If original_nets is set to True it also returns the original non-aggregated network.\"\"\"\n",
    "    \n",
    "    #Reading the data and setting t0\n",
    "    f = open(data_dir+'/tij_' + dataset +'.dat')\n",
    "    (t0,i,j) = map(int,str.split(f.readline()))\n",
    "    #Special temporal scale for these two Datasets\n",
    "    if dataset not in ['LyonSchool','LH10']:\n",
    "        t0 = t0*20\n",
    "    f.close()\n",
    "    \n",
    "    #Aggregation on scale of x minutes\n",
    "    delta_t = 20;#*3*n_minutes;   \n",
    "    if original_nets==True:\n",
    "        originalnetworks = {}\n",
    "    aggnetworks = {}\n",
    "    f = open(data_dir+'/tij_' + dataset +'.dat')\n",
    "    aggtime = t0\n",
    "    for line in f:\n",
    "        (t,i,j) = map(int,str.split(line))\n",
    "        #Special temporal scale for these two Datasets\n",
    "        if dataset not in ['LyonSchool','LH10']:\n",
    "            t = t*20\n",
    "        if original_nets==True:\n",
    "            if t not in originalnetworks:\n",
    "                originalnetworks[t] = nx.Graph()\n",
    "            originalnetworks[t].add_edge(i,j)\n",
    "        #this is a trick using the integer division in python\n",
    "        aggtime = aggtime + delta_t#((t-t0)/delta_t)*delta_t \n",
    "        if aggtime not in aggnetworks:\n",
    "            aggnetworks[aggtime] = nx.Graph()\n",
    "        aggnetworks[aggtime].add_edge(i,j)\n",
    "    f.close();\n",
    "    if original_nets==True:\n",
    "        return originalnetworks, aggnetworks;\n",
    "    else:\n",
    "        return aggnetworks;\n",
    "    \n",
    "def extract_cliques(gs):\n",
    "    listsaggcliques = {}\n",
    "    #looping over the networks in temporal order\n",
    "    for t in sorted(gs.keys()):\n",
    "        listsaggcliques[t] = list(nx.find_cliques(gs[t]));\n",
    "    #returning a dictionary with list of cliques as values\n",
    "    return listsaggcliques;\n",
    "    \n",
    "def clique_weights(cliques):\n",
    "    from collections import Counter;\n",
    "    tot_c = [];\n",
    "    for t in cliques:\n",
    "        tot_c.extend(map(frozenset,cliques[t]))\n",
    "    return Counter(tot_c);\n",
    "\n",
    "def average_clique_size(ws):\n",
    "    return np.sum(map(lambda x: 1.0 * ws[x] * len(x), ws.keys()))/np.sum(ws.values());\n",
    "\n",
    "def clean_non_maximal(ws):\n",
    "    sd = dict(zip(ws.keys(), map(len,ws.keys())));\n",
    "    import operator\n",
    "    sizes = set(map(len,ws.keys()));\n",
    "    sorted_sd = sorted(sd.items(), key=operator.itemgetter(1));\n",
    "    simplices = dict.fromkeys(list(sizes),[]);\n",
    "    maximal_simplices = {};\n",
    "    for x in ws:\n",
    "        maximal = True;\n",
    "        for xx in ws:\n",
    "            if (len(x)<len(xx)):\n",
    "                if (set(x)<set(xx)):\n",
    "                    maximal=False;\n",
    "                    break;\n",
    "        if maximal:\n",
    "            maximal_simplices[x] = ws[x];\n",
    "    return maximal_simplices;\n",
    "\n",
    "def save_cliques(ws, data_dir, dataset,n_minutes, thr=None):\n",
    "    if thr==None:\n",
    "        ls = map(list,ws.keys());\n",
    "    else:\n",
    "        ls = [list(x) for x in ws if ws[x]>=thr];\n",
    "    i_file = data_dir+'aggr_'+str(n_minutes)+'min_cliques_thr'+str(thr)+'_'+dataset+'.json';\n",
    "    o_file = data_dir+'aggr_'+str(n_minutes)+'min_cliques_thr'+str(thr)+'_'+dataset+'.hg';\n",
    "    jd = open(i_file,'w')\n",
    "    json.dump(ls,jd)\n",
    "    jd.close()\n",
    "    command = \"sed -e 's/^.//' -e 's/.$//' \"+i_file+\">\"+ o_file\n",
    "    os.system(command);\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../../data/Sociopatterns/Originaldata_20s/' \n",
    "out_dir = '../../data/Sociopatterns/Processed_data/' \n",
    "\n",
    "datasets = ['InVS13','InVS15','LH10','LyonSchool','SFHH','Thiers13']\n",
    "#datasets = ['LyonSchool']\n",
    "n_minutes = 0.33\n",
    "thrs = [1,3,5]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for thr in thrs:\n",
    "        aggs = extract_networks(dataset_dir, dataset, n_minutes, original_nets=False);\n",
    "        cliques = extract_cliques(aggs)\n",
    "        ws = clique_weights(cliques);\n",
    "        maximal_cliques = clean_non_maximal(ws);\n",
    "        save_cliques(maximal_cliques, out_dir, dataset, n_minutes, thr=thr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
